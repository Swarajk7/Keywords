{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "concrete-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wrapped-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = 'khadanga_crawl'\n",
    "COLLECTION_NAME = 'blogs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "technical-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_uri = \"mongodb+srv://<usr>:<pwd>@cluster0.kf3n4.mongodb.net/{}?authSource=admin&ssl=true\".format(DB_NAME)\n",
    "conn = MongoClient(mongo_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "later-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = conn.get_default_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cubic-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_blogs = database[COLLECTION_NAME].find({\"link\":\"https://arxiv.org/pdf/0704.0002\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "processed-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs = []\n",
    "for blog in all_blogs:\n",
    "    blogs.append(blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unavailable-prisoner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total count = 1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Total count = {}\".format(len(blogs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "laden-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "def body(b):\n",
    "    return \"{}\".format(b['body'])\n",
    "def title(b):\n",
    "    return \"{}\".format(b['title'])\n",
    "def abstract(x):\n",
    "    return x['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "grave-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "import textacy.ke\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "settled-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Keywords = namedtuple('Keywords', ['title', 'body', 'abstract'])\n",
    "\n",
    "def get_keywords(blog):\n",
    "    tk, bk, ak = None,None,None\n",
    "    if 'title' in blog:\n",
    "        title_doc = textacy.make_spacy_doc(title(blog), lang='en')\n",
    "        tk = textacy.ke.sgrank(title_doc, ngrams=(1,2,3), normalize=\"lower\", include_pos=['NOUN', 'PROPN'])\n",
    "    if 'body' in blog:\n",
    "        body_doc = textacy.make_spacy_doc(body(blog), lang='en')\n",
    "        bk = textacy.ke.sgrank(body_doc, ngrams=(1,2,3), normalize=\"lower\", topn=5, include_pos=['NOUN', 'PROPN'])\n",
    "    if 'abstract' in blog:\n",
    "        abstract_doc = textacy.make_spacy_doc(abstract(blog), lang='en')\n",
    "        ak = textacy.ke.sgrank(abstract_doc, ngrams=(1,2,3), normalize=\"lower\", topn = 5,include_pos=['NOUN', 'PROPN'])        \n",
    "    keywords = Keywords(tk,bk,ak)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "sweet-anthony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepVariant: Highly Accurate Genomes With Deep Neural Networks\n",
      "title\n",
      " neural networks\n",
      " genomes\n",
      " deepvariant\n",
      "body\n",
      " reference genomes\n",
      " hts\n",
      " genomics\n",
      " image classification\n",
      " instruments\n",
      " brain\n",
      " source\n",
      " thegoogle\n",
      " reads\n",
      " data\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "blog = random.choice(blogs)\n",
    "keywords = get_keywords(blog)\n",
    "print(title(blog))\n",
    "print(\"{}\\n {}\".format(\"title\", \"\\n \".join(map(lambda x: x[0],keywords.title))))\n",
    "print(\"{}\\n {}\".format(\"body\", \"\\n \".join(map(lambda x: x[0],keywords.body))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "harmful-moisture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posted by Mark DePristo and Ryan Poplin, Google Brain Team(Crossposted on theGoogle Open Source Blog)Across many scientific disciplines, but in particular in the field of genomics, major breakthroughs have often resulted from new technologies. FromSanger sequencing, which made it possible to sequence the human genome, to themicroarray technologiesthat enabled the first large-scale genome-wide experiments, new instruments and tools have allowed us to look ever more deeply into the genome and apply the results broadly tohealth,agricultureandecology.One of the most transformative new technologies in genomics was high-throughput sequencing (HTS), which first became commercially available in the early 2000s. HTS allowed scientists and clinicians to produce sequencing data quickly, cheaply, and at scale. However, the output of HTS instruments is not the genome sequence for the individual being analyzed â€” for humans this is 3 billion paired bases (guanine, cytosine, adenine and thymine) organized into 23 pairs of chromosomes. Instead, these instruments generate ~1 billion short sequences, known as reads. Each read represents just 100 of the 3 billion bases, and per-base error rates range from 0.1-10%. Processing the HTS output into a single, accurate and complete genome sequence is a major outstanding challenge. The importance of this problem, for biomedical applications in particular, has motivated efforts such as theGenome in a Bottle Consortium(GIAB), which produces high confidence human reference genomes that can be used for validation and benchmarking, as well as theprecisionFDAcommunity challenges, which are designed to foster innovation that will improve the quality and accuracy of HTS-based genomic tests.For any given location in the genome, there are multiple reads among the ~1 billion that include a base at that position. Each read is aligned to a reference, and then each of the bases in the read is compared to the base of the reference at that location. When a read includes a base that differs from the reference, it may indicate a variant (a difference in the true sequence), or it may be an error.Today, we announce theopen source release of DeepVariant, a deep learning technology to reconstruct the true genome sequence from HTS sequencer data with significantly greater accuracy than previous classical methods. This work is the product of more than two years of research by theGoogle Brain team, in collaboration withVerily Life Sciences. DeepVariant transforms the task of variant calling, as this reconstruction problem is known in genomics, into an image classification problem well-suited to Google'sexisting technology and expertise.Each of the four images above is a visualization of actual sequencer reads aligned to a reference genome. A key question is how to use the reads to determine whether there is a variant on both chromosomes, on just one chromosome, or on neither chromosome. There is more than one type of variant, withSNPsandinsertions/deletionsbeing the most common. A: a true SNP on one chromosome pair, B: a deletion on one chromosome, C: a deletion on both chromosomes, D: a false variant caused by errors. It's easy to see that these look quite distinct when visualized in this manner.We started with GIAB reference genomes, for which there is high-quality ground truth (or the closest approximation currently possible). Using multiple replicates of these genomes, we produced tens of millions of training examples in the form of multi-channel tensors encoding the HTS instrument data, and then trained a TensorFlow-based image classification model to identify the true genome sequence from the experimental data produced by the instruments. Although the resulting deep learning model,DeepVariant, had no specialized knowledge about genomics or HTS, within a year it had won the the highestSNPaccuracy award at theprecisionFDA Truth Challenge, outperforming state-of-the-art methods. Since then, we've further reduced the error rate by more than 50%.DeepVariant is being released as open source software to encourage collaboration and to accelerate the use of this technology to solve real world problems. To further this goal, we partnered withGoogle Cloud Platform(GCP) to deployDeepVariant workflowson GCP, available today, in configurations optimized for low-cost and fast turnarounds using scalable GCP technologies like thePipelines API. This paired set of releases provides a smooth ramp for users to explore and evaluate the capabilities of DeepVariant in their current compute environment while providing a scalable, cloud-based solution to satisfy the needs of even the largest genomics datasets.DeepVariant is the first of what we hope will be many contributions that leverage Google's computing infrastructure and ML expertise to both better understand the genome and to provide deep learning-based genomics tools to the community. This is all part of a broader goal to apply Google technologies tohealthcareand otherscientific applications, and to make the results of these efforts broadly accessible.\n"
     ]
    }
   ],
   "source": [
    "print(body(blog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "accurate-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "excessive-likelihood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "this is a statement in english"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"this is a statement in english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "domestic-motion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Handwriting Input in 82 languages on your Android mobile device\n"
     ]
    }
   ],
   "source": [
    "print(title(blogs[49]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fifteen-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = textacy.make_spacy_doc(title(blogs[49]), lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cross-impression",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doc(11 tokens: \"Google Handwriting Input in 82 languages on you...\")'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "civic-orange",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('networks', 0.6392318376935061),\n",
       " ('quantum', 0.7493746385669857),\n",
       " ('neural', 0.7493746385669857),\n",
       " ('neural networks', 0.919474686762359),\n",
       " ('quantum neural', 1.2808260138096879),\n",
       " ('quantum neural networks', 1.809988089918808)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textacy.ke.yake(\n",
    "    doc, \n",
    "    normalize=\"lower\", \n",
    "    ngrams=(1,2,3), \n",
    "    window_size=2, \n",
    "    include_pos=['NOUN', 'PROPN']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "finished-bacon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quantum neural networks', 2.1666666666666665)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textacy.ke.scake(\n",
    "    doc, \n",
    "    normalize=\"lower\", \n",
    "    include_pos=['NOUN', 'PROPN']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "classical-reverse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textacy.ke.sgrank(\n",
    "    doc, \n",
    "    ngrams=(1,2,3), \n",
    "    normalize=\"lower\", \n",
    "    include_pos=['NOUN', 'PROPN']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "architectural-constitutional",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quantum neural networks', 0.8245616022774844)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textacy.ke.textrank(\n",
    "    doc, \n",
    "    normalize=\"lower\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-fantasy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "shared-stephen",
   "metadata": {},
   "source": [
    "### Attach keywords to Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "conventional-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = 'TechVault'\n",
    "COLLECTION_NAME = 'paper'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "wicked-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_uri = \"mongodb+srv://<usr>:<pwd>@cluster0.kf3n4.mongodb.net/{}?authSource=admin&ssl=true\".format(DB_NAME)\n",
    "conn = MongoClient(mongo_uri)\n",
    "database = conn.get_default_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "professional-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_blogs = database[COLLECTION_NAME].find({\"link\":\"https://arxiv.org/pdf/0704.0046\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "important-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = []\n",
    "for paper in all_blogs:\n",
    "    papers.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "republican-reference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': ObjectId('60161bc2dc518a69be4aa445'),\n",
       "  'date': '2009-11-13',\n",
       "  'total_comments': 0,\n",
       "  'link': 'https://arxiv.org/pdf/0704.0046',\n",
       "  'contentId': 'edd3cd8287974dd885d8c4d70ab19df9',\n",
       "  'abstract': '  In a quantum mechanical model, Diosi, Feldmann and Kosloff arrived at a\\nconjecture stating that the limit of the entropy of certain mixtures is the\\nrelative entropy as system size goes to infinity. The conjecture is proven in\\nthis paper for density matrices. The first proof is analytic and uses the\\nquantum law of large numbers. The second one clarifies the relation to channel\\ncapacity per unit cost for classical-quantum channels. Both proofs lead to\\ngeneralization of the conjecture.\\n',\n",
       "  'title': 'A limit relation for entropy and channel capacity per unit cost',\n",
       "  'total_likes': 0,\n",
       "  'authors': 'I. Csiszar, F. Hiai and D. Petz'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "advanced-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list_of_keywords(k):\n",
    "    items = []\n",
    "    if k.title:\n",
    "        items += list(map(lambda x:x[0], k.title))\n",
    "    if k.abstract:\n",
    "        items += list(map(lambda x:x[0], k.abstract))\n",
    "    return list(set(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "respective-dubai",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conjecture',\n",
       " 'channel capacity',\n",
       " 'unit cost',\n",
       " 'system size',\n",
       " 'limit relation',\n",
       " 'density matrices',\n",
       " 'quantum channels']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_list_of_keywords(get_keywords(papers[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "widespread-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import UpdateOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers = database[COLLECTION_NAME].find().batch_size(50)\n",
    "try:\n",
    "    update_operations = []\n",
    "    count = 0\n",
    "    for paper in all_papers:\n",
    "        operation = UpdateOne(\n",
    "            {'_id': paper['_id']}, \n",
    "            {'$set': {'keywords':clean_list_of_keywords(get_keywords(paper))}}\n",
    "        )\n",
    "        update_operations.append(operation)\n",
    "        if len(update_operations) == 500:\n",
    "            # update here\n",
    "            database[COLLECTION_NAME].bulk_write(update_operations, ordered=False)\n",
    "            update_operations = []\n",
    "            count += 500\n",
    "            print(count)\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "finally:\n",
    "    all_papers.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
